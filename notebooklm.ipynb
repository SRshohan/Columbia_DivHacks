{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the OpenAI API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Ensure the key was loaded correctly\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"API key not found. Please set it in the .env file.\")\n",
    "else:\n",
    "    print(openai_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot Response:  It may also be helpful to talk to a trusted friend or family member about what is causing your anxiety. Remember to take care of yourself and prioritize your mental health. If your anxiety persists, it may be beneficial to seek professional help from a therapist or counselor. You are not alone and there are resources available to support you in managing your anxiety.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "# Custom responses for different emotions\n",
    "emotion_responses = {\n",
    "    \"anxiety\": \"Feeling anxious can be tough. Consider practicing some breathing exercises or taking a walk to clear your mind.\",\n",
    "    \"depression\": \"I’m sorry to hear you’re feeling this way. It's important to reach out to someone who can help, like a mental health professional.\",\n",
    "    \"sadness\": \"It's okay to feel sad sometimes. Try to engage in activities that bring you joy or comfort.\",\n",
    "    \"loneliness\": \"Feeling lonely can be challenging. Consider reaching out to a friend or engaging in a hobby you enjoy.\"\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the OpenAI LLM with the GPT-O1 model\n",
    "llm = OpenAI(temperature=0.7, openai_api_key=openai_api_key)\n",
    "\n",
    "def detect_emotion(user_input):\n",
    "    # Placeholder for emotion detection logic\n",
    "    # For now, return a default emotion for testing\n",
    "    return \"anxiety\"  # Replace with actual emotion detection logic\n",
    "\n",
    "def generate_response(user_input):\n",
    "    detected_emotion = detect_emotion(user_input)\n",
    "\n",
    "    if detected_emotion in emotion_responses:\n",
    "        advice = emotion_responses[detected_emotion]\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"emotion\", \"user_input\"],\n",
    "            template=\"You are a mental health chatbot. The user is feeling {emotion}. Based on this, provide advice: {user_input}.\"\n",
    "        )\n",
    "        response = llm(prompt.format(emotion=detected_emotion, user_input=advice))\n",
    "    else:\n",
    "        response = \"I'm here to listen. Can you tell me more about how you're feeling?\"\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example user input\n",
    "user_input = \"I've been feeling so anxious and overwhelmed lately.\"\n",
    "response = generate_response(user_input)\n",
    "print(f\"Chatbot Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot Response: \n",
      "\n",
      "Hello there, I am here to listen and support you. I can understand how overwhelming and difficult it can be to deal with anxiety. It takes a lot of courage to share your feelings and I commend you for that.\n",
      "\n",
      "It sounds like you have been feeling anxious and overwhelmed lately. That can be a tough and exhausting experience. Have you tried practicing some breathing exercises or taking a walk to clear your mind? These can be helpful in managing anxiety and giving yourself a break from overwhelming thoughts.\n",
      "\n",
      "Remember, it's important to take care of yourself and prioritize your mental well-being. You are not alone in this and there are many resources and techniques available to help you cope with anxiety. I am here for you whenever you need a listening ear or some support.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Custom responses for different emotions\n",
    "emotion_responses = {\n",
    "    \"anxiety\": \"Feeling anxious can be tough. Consider practicing some breathing exercises or taking a walk to clear your mind.\",\n",
    "    \"depression\": \"I’m sorry to hear you’re feeling this way. It's important to reach out to someone who can help, like a mental health professional.\",\n",
    "    \"sadness\": \"It's okay to feel sad sometimes. Try to engage in activities that bring you joy or comfort.\",\n",
    "    \"loneliness\": \"Feeling lonely can be challenging. Consider reaching out to a friend or engaging in a hobby you enjoy.\"\n",
    "}\n",
    "\n",
    "\n",
    "# Initial conversation context prompt\n",
    "initial_prompt = (\n",
    "    \"You are an emotional AI companion using Langchain with memory capabilities. \"\n",
    "    \"Your goal is to create a friendly and supportive environment for users to express their feelings and gather specific information about their emotional state. \"\n",
    "    \"Ensure that you maintain the context of the conversation to support long-term interactions. \"\n",
    "    \"Start with a warm greeting and encourage the user to share their feelings.\"\n",
    ")\n",
    "\n",
    "def detect_emotion(user_input):\n",
    "    # Placeholder for emotion detection logic\n",
    "    # For now, return a default emotion for testing\n",
    "    return \"anxiety\"  # Replace with actual emotion detection logic\n",
    "\n",
    "def generate_response(user_input, memory):\n",
    "    # Use the initial prompt to set the context for the conversation\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"emotion\", \"user_input\"],\n",
    "        template=\"{context} The user says: {user_input}. Based on this, respond empathetically and provide advice based on their emotional state.\"\n",
    "    )\n",
    "    \n",
    "    detected_emotion = detect_emotion(user_input)\n",
    "    \n",
    "    # Store the user's emotional state in memory (you'll implement a memory system separately)\n",
    "    memory[\"emotion\"] = detected_emotion\n",
    "    \n",
    "    # Generate the advice based on the detected emotion\n",
    "    if detected_emotion in emotion_responses:\n",
    "        advice = emotion_responses[detected_emotion]\n",
    "        # Add advice to the response context\n",
    "        context = initial_prompt + f\" The user is feeling {detected_emotion}. Advice: {advice}\"\n",
    "        response = llm(prompt.format(context=context, emotion=detected_emotion, user_input=user_input))\n",
    "    else:\n",
    "        context = initial_prompt + \" I'm here to listen. Can you tell me more about how you're feeling?\"\n",
    "        response = llm(prompt.format(context=context, emotion=\"\", user_input=user_input))\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example user input\n",
    "memory = {}  # Initialize a memory dictionary to store user context\n",
    "user_input = \"I've been feeling so anxious and overwhelmed lately.\"\n",
    "response = generate_response(user_input, memory)\n",
    "print(f\"Chatbot Response: {response}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EmotionExerciseEnv.reset() got an unexpected keyword argument 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust the number of timesteps as needed\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m     74\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_emotion_exercise_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:287\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[1;32m    278\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    284\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOnPolicyAlgorithm:\n\u001b[1;32m    285\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 287\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/common/base_class.py:423\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:77\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m     76\u001b[0m     maybe_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx]} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx] \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m---> 77\u001b[0m     obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_seeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmaybe_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Seeds and options are only used once\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/common/monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_reset_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: EmotionExerciseEnv.reset() got an unexpected keyword argument 'seed'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Define your custom environment\n",
    "class EmotionExerciseEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(EmotionExerciseEnv, self).__init__()\n",
    "\n",
    "        # Observation space: [emotion intensity, physical condition, fatigue level]\n",
    "        self.observation_space = spaces.Box(low=np.array([0, 0, 0]), \n",
    "                                             high=np.array([1, 1, 1]), \n",
    "                                             dtype=np.float32)\n",
    "\n",
    "        # Define the action space (0: Exercise, 1: Song, 2: Quote)\n",
    "        self.action_space = spaces.Discrete(3)  # Three actions: exercise, song, quote\n",
    "\n",
    "        # Initialize state variables\n",
    "        self.state = None\n",
    "        self.exercises = [\"Jumping Jacks\", \"Push-ups\", \"Squats\"]\n",
    "        self.songs = [\"Happy Song\", \"Chill Beats\", \"Motivational Anthem\"]\n",
    "        self.quotes = [\"Keep going!\", \"You can do it!\", \"Believe in yourself!\"]\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset state to random initial conditions\n",
    "        self.state = np.random.uniform(low=0, high=1, size=(3,)).astype(np.float32)  # Randomized initial state\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Suggest based on action\n",
    "        if action == 0:  # Exercise\n",
    "            suggestion = np.random.choice(self.exercises)\n",
    "        elif action == 1:  # Song\n",
    "            suggestion = np.random.choice(self.songs)\n",
    "        elif action == 2:  # Quote\n",
    "            suggestion = np.random.choice(self.quotes)\n",
    "\n",
    "        # Collect user feedback\n",
    "        user_feedback = self.get_user_feedback(suggestion)\n",
    "\n",
    "        # Calculate reward based on user feedback\n",
    "        reward = self.calculate_reward(user_feedback)\n",
    "\n",
    "        # Update the state to simulate an observation (e.g., user emotional state changes)\n",
    "        self.state = np.random.uniform(low=0, high=1, size=(3,)).astype(np.float32)  # Randomized state update\n",
    "\n",
    "        done = False  # Define when to end the episode if necessary\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def calculate_reward(self, feedback):\n",
    "        if feedback == \"yes\":\n",
    "            return 1.0  # Positive feedback\n",
    "        elif feedback == \"no\":\n",
    "            return -1.0  # Negative feedback\n",
    "        else:\n",
    "            return 0.0  # Neutral feedback\n",
    "\n",
    "    def get_user_feedback(self, suggestion):\n",
    "        print(f\"Suggestion: {suggestion}. Did this help you feel better? (yes/no)\")\n",
    "        feedback = input().strip().lower()\n",
    "        return feedback\n",
    "\n",
    "# Set up the custom environment\n",
    "env = EmotionExerciseEnv()\n",
    "\n",
    "# Create and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=10000)  # Adjust the number of timesteps as needed\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"ppo_emotion_exercise_model\")\n",
    "\n",
    "print(\"Training complete! Model saved as 'ppo_emotion_exercise_model'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.4     |\n",
      "|    ep_rew_mean     | 23.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 27869    |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 35.6        |\n",
      "|    ep_rew_mean          | 35.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9687        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014252773 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | 0.0033      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.61        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 20.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 77.7        |\n",
      "|    ep_rew_mean          | 77.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 8557        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015066555 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.646      |\n",
      "|    explained_variance   | 0.386       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0333     |\n",
      "|    value_loss           | 27.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 113         |\n",
      "|    ep_rew_mean          | 113         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7810        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012420356 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.596      |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.7        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    value_loss           | 43.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 03:09:01.285 Python[32696:11356683] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-10-06 03:09:01.285 Python[32696:11356683] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)\n\u001b[1;32m     20\u001b[0m obs, rewards, dones, info \u001b[38;5;241m=\u001b[39m vec_env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mvec_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:103\u001b[0m, in \u001b[0;36mDummyVecEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Gym environment rendering. If there are multiple environments then\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    they are tiled together in one image via ``BaseVecEnv.render()``.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    :param mode: The rendering type.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:264\u001b[0m, in \u001b[0;36mVecEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvecenv\u001b[39m\u001b[38;5;124m\"\u001b[39m, bigimg[:, :, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 264\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bigimg\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Parallel environments\n",
    "vec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"ppo_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = PPO.load(\"ppo_cartpole\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EmotionExerciseEnv.reset() got an unexpected keyword argument 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     53\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, vec_env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_emotion_exercise\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Load and run the model\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:287\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[1;32m    278\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    284\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOnPolicyAlgorithm:\n\u001b[1;32m    285\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 287\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/common/base_class.py:423\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:77\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m     76\u001b[0m     maybe_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx]} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx] \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m---> 77\u001b[0m     obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_seeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmaybe_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Seeds and options are only used once\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/common/monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_reset_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: EmotionExerciseEnv.reset() got an unexpected keyword argument 'seed'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Define your custom environment\n",
    "class EmotionExerciseEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(EmotionExerciseEnv, self).__init__()\n",
    "        self.observation_space = spaces.Box(low=np.array([0, 0, 0]), high=np.array([1, 1, 1]), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(3)  # Three actions: exercise, song, quote\n",
    "        self.state = None\n",
    "        self.exercises = [\"Jumping Jacks\", \"Push-ups\", \"Squats\"]\n",
    "        self.songs = [\"Happy Song\", \"Chill Beats\", \"Motivational Anthem\"]\n",
    "        self.quotes = [\"Keep going!\", \"You can do it!\", \"Believe in yourself!\"]\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(low=0, high=1, size=(3,)).astype(np.float32)  # Randomized initial state\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            suggestion = np.random.choice(self.exercises)\n",
    "        elif action == 1:\n",
    "            suggestion = np.random.choice(self.songs)\n",
    "        else:\n",
    "            suggestion = np.random.choice(self.quotes)\n",
    "\n",
    "        user_feedback = self.get_user_feedback(suggestion)  # Implement feedback collection\n",
    "        reward = self.calculate_reward(user_feedback)\n",
    "\n",
    "        self.state = np.random.uniform(low=0, high=1, size=(3,)).astype(np.float32)  # Simulated state update\n",
    "        done = False\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def calculate_reward(self, feedback):\n",
    "        if feedback == \"yes\":\n",
    "            return 1.0\n",
    "        elif feedback == \"no\":\n",
    "            return -1.0\n",
    "        return 0.0\n",
    "\n",
    "    def get_user_feedback(self, suggestion):\n",
    "        print(f\"Suggestion: {suggestion}. Did this help you feel better? (yes/no)\")\n",
    "        feedback = input().strip().lower()\n",
    "        return feedback\n",
    "\n",
    "# Set up vectorized environments\n",
    "vec_env = make_vec_env(EmotionExerciseEnv, n_envs=4)\n",
    "\n",
    "# Train the model\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"ppo_emotion_exercise\")\n",
    "\n",
    "# Load and run the model\n",
    "del model  # Demonstrate saving and loading\n",
    "model = PPO.load(\"ppo_emotion_exercise\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render()  # Optional: Implement render if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class EmotionExerciseEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(EmotionExerciseEnv, self).__init__()\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(3,), dtype=np.float32)  # Example observation space\n",
    "        self.action_space = spaces.Discrete(3)  # Three actions: exercise, song, quote\n",
    "        self.exercises = [\"Jumping Jacks\", \"Push-ups\", \"Squats\"]\n",
    "        self.songs = [\"Happy Song\", \"Chill Beats\", \"Motivational Anthem\"]\n",
    "        self.quotes = [\"Keep going!\", \"You can do it!\", \"Believe in yourself!\"]\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(low=0, high=1, size=(3,)).astype(np.float32)  # Random initial state\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            suggestion = np.random.choice(self.exercises)\n",
    "        elif action == 1:\n",
    "            suggestion = np.random.choice(self.songs)\n",
    "        else:\n",
    "            suggestion = np.random.choice(self.quotes)\n",
    "\n",
    "        user_feedback = self.get_user_feedback(suggestion)  # Implement feedback collection\n",
    "        reward = self.calculate_reward(user_feedback)\n",
    "\n",
    "        self.state = np.random.uniform(low=0, high=1, size=(3,)).astype(np.float32)  # Simulated state update\n",
    "        done = False\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def calculate_reward(self, feedback):\n",
    "        if feedback == \"yes\":\n",
    "            return 1.0\n",
    "        elif feedback == \"no\":\n",
    "            return -1.0\n",
    "        return 0.0\n",
    "\n",
    "    def get_user_feedback(self, suggestion):\n",
    "        print(f\"Suggestion: {suggestion}. Did this help you feel better? (yes/no)\")\n",
    "        feedback = input().strip().lower()\n",
    "        return feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggestion: Chill Beats. Did this help you feel better? (yes/no)\n",
      "Suggestion: Push-ups. Did this help you feel better? (yes/no)\n",
      "Suggestion: Squats. Did this help you feel better? (yes/no)\n",
      "Suggestion: Motivational Anthem. Did this help you feel better? (yes/no)\n",
      "Suggestion: Squats. Did this help you feel better? (yes/no)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "class EmotionExerciseEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(EmotionExerciseEnv, self).__init__()\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(3,), dtype=np.float32)  # Example observation space\n",
    "        self.action_space = spaces.Discrete(3)  # Three actions: exercise, song, quote\n",
    "        self.exercises = [\"Jumping Jacks\", \"Push-ups\", \"Squats\"]\n",
    "        self.songs = [\"Happy Song\", \"Chill Beats\", \"Motivational Anthem\"]\n",
    "        self.quotes = [\"Keep going!\", \"You can do it!\", \"Believe in yourself!\"]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.state = np.random.uniform(low=0, high=1, size=(3,)).astype(np.float32)  # Random initial state\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            suggestion = np.random.choice(self.exercises)\n",
    "        elif action == 1:\n",
    "            suggestion = np.random.choice(self.songs)\n",
    "        else:\n",
    "            suggestion = np.random.choice(self.quotes)\n",
    "\n",
    "        user_feedback = self.get_user_feedback(suggestion)  # Implement feedback collection\n",
    "        reward = self.calculate_reward(user_feedback)\n",
    "\n",
    "        self.state = np.random.uniform(low=0, high=1, size=(3,)).astype(np.float32)  # Simulated state update\n",
    "        done = False\n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "    def calculate_reward(self, feedback):\n",
    "        if feedback == \"yes\":\n",
    "            return 1.0\n",
    "        elif feedback == \"no\":\n",
    "            return -1.0\n",
    "        return 0.0\n",
    "\n",
    "    def get_user_feedback(self, suggestion):\n",
    "        print(f\"Suggestion: {suggestion}. Did this help you feel better? (yes/no)\")\n",
    "        feedback = input().strip().lower()\n",
    "        return feedback\n",
    "\n",
    "# Function to interact with the user and update the model\n",
    "def interact_and_train(model, env, total_timesteps=10000):\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(total_timesteps):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        model.learn(total_timesteps=1)  # Update the model with the new experience\n",
    "        if done:\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "# Train the model with user interaction\n",
    "interact_and_train(model, env, total_timesteps=10000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_emotion_exercise\")\n",
    "\n",
    "\n",
    "# Check the environment\n",
    "env = EmotionExerciseEnv()\n",
    "check_env(env)\n",
    "\n",
    "# Train the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_emotion_exercise\")\n",
    "\n",
    "# Load the model\n",
    "model = PPO.load(\"ppo_emotion_exercise\")\n",
    "\n",
    "# Evaluate the model\n",
    "obs, _ = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'setiment_analysis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPO\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv_checker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_env\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msetiment_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect_sentiment\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Custom environment for emotion exercise\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEmotionExerciseEnv\u001b[39;00m(gym\u001b[38;5;241m.\u001b[39mEnv):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'setiment_analysis'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from setiment_analysis import detect_sentiment\n",
    "\n",
    "# Custom environment for emotion exercise\n",
    "class EmotionExerciseEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(EmotionExerciseEnv, self).__init__()\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(3,), dtype=np.float32)  # Example observation space\n",
    "        self.action_space = spaces.Discrete(3)  # Three actions: exercise, song, quote\n",
    "        self.exercises = [\"Jumping Jacks\", \"Push-ups\", \"Squats\"]\n",
    "        self.songs = [\"Happy Song\", \"Chill Beats\", \"Motivational Anthem\"]\n",
    "        self.quotes = [\"Keep going!\", \"You can do it!\", \"Believe in yourself!\"]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.state = np.random.uniform(low=0, high=1, size=(3,)).astype(np.float32)  # Random initial state\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            suggestion = np.random.choice(self.exercises)\n",
    "        elif action == 1:\n",
    "            suggestion = np.random.choice(self.songs)\n",
    "        else:\n",
    "            suggestion = np.random.choice(self.quotes)\n",
    "\n",
    "        user_feedback = self.get_user_feedback(suggestion)\n",
    "        reward = self.calculate_reward(user_feedback)\n",
    "\n",
    "        self.state = np.random.uniform(low=0, high=1, size=(3,)).astype(np.float32)\n",
    "        done = False\n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "    def calculate_reward(self, feedback):\n",
    "        if feedback == \"yes\":\n",
    "            return 1.0\n",
    "        elif feedback == \"no\":\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def get_user_feedback(self, suggestion):\n",
    "        # Simulate user feedback\n",
    "        if np.random.rand() > 0.5:\n",
    "            return \"yes\"\n",
    "        else:\n",
    "            return \"no\"\n",
    "\n",
    "# Function to interact with the user and update the model\n",
    "def interact_and_train(model, env, total_timesteps=10000):\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(total_timesteps):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        model.learn(total_timesteps=100)  # Update the model with the new experience\n",
    "        if done:\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "# Function to load and evaluate the model\n",
    "def load_and_evaluate_model(model_path, env, num_steps=1000):\n",
    "    # Load the model\n",
    "    model = PPO.load(model_path)\n",
    "\n",
    "    # Evaluate the model\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(num_steps):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, done, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "class Chatbot(Resource):\n",
    "    def post(self):\n",
    "        parser = reqparse.RequestParser()\n",
    "        parser.add_argument('user_input', type=str, required=True, help=\"Input cannot be blank!\")\n",
    "        args = parser.parse_args()\n",
    "\n",
    "        user_input = args[\"user_input\"]\n",
    "        sentiment = detect_sentiment(user_input)\n",
    "\n",
    "        # RL agent action\n",
    "        env = EmotionExerciseEnv()\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "        # Chatbot response\n",
    "        chatbot_response = generate_response(user_input, sentiment)\n",
    "\n",
    "        model.learn(total_timesteps=100)  # Update the model with the new experience\n",
    "\n",
    "        # Save the model\n",
    "        model.save(\"ppo_emotion_exercise\")\n",
    "\n",
    "        # Load the model\n",
    "        model = PPO.load(\"ppo_emotion_exercise\")\n",
    "\n",
    "        # Evaluate the model\n",
    "        load_and_evaluate_model(\"ppo_emotion_exercise\", env, num_steps=1000)\n",
    "\n",
    "        return {'response': chatbot_response}\n",
    "from flask import Flask\n",
    "from flask_restful import Api\n",
    "from chat_endpoint import Chatbot  # Adjust this import based on your file structure\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "api = Api(app)\n",
    "CORS(app)\n",
    "api = Api(app)\n",
    "\n",
    "# Register the Chatbot resource with the API\n",
    "api.add_resource(Chatbot, '/')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 90\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Episode Length over \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m episodes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Load and evaluate the model\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m \u001b[43mload_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43memotion_exercise_ppo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 74\u001b[0m, in \u001b[0;36mload_and_evaluate_model\u001b[0;34m(model_path, env, num_episodes)\u001b[0m\n\u001b[1;32m     71\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 74\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     76\u001b[0m     episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/common/base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/common/policies.py:368\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 368\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[1;32m    370\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/common/policies.py:717\u001b[0m, in \u001b[0;36mActorCriticPolicy._predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    710\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;124;03m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;124;03m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/common/policies.py:752\u001b[0m, in \u001b[0;36mActorCriticPolicy.get_distribution\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    750\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpi_features_extractor)\n\u001b[1;32m    751\u001b[0m latent_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_actor(features)\n\u001b[0;32m--> 752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_action_dist_from_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/stable_baselines3/common/policies.py:691\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_action_dist_from_latent\u001b[0;34m(self, latent_pi)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_action_dist_from_latent\u001b[39m(\u001b[38;5;28mself\u001b[39m, latent_pi: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Distribution:\n\u001b[1;32m    685\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;124;03m    Retrieve action distribution given the latent codes.\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m    :param latent_pi: Latent code for the actor\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;124;03m    :return: Action distribution\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 691\u001b[0m     mean_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, DiagGaussianDistribution):\n\u001b[1;32m    694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(mean_actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std)\n",
      "File \u001b[0;32m~/Desktop/Projects/Hackathon_DivHacks/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1549\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1546\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1551\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# Custom environment for emotion exercise\n",
    "class EmotionExerciseEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(EmotionExerciseEnv, self).__init__()\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(3,), dtype=np.float32)  # Example observation space\n",
    "        self.action_space = spaces.Discrete(3)  # Three actions: exercise, song, quote\n",
    "        self.exercises = [\"Jumping Jacks\", \"Push-ups\", \"Squats\"]\n",
    "        self.songs = [\"Happy Song\", \"Chill Beats\", \"Motivational Anthem\"]\n",
    "        self.quotes = [\"Keep going!\", \"You can do it!\", \"Believe in yourself!\"]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.state = np.random.uniform(low=0, high=1, size=(3,)).astype(np.float32)  # Random initial state\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            suggestion = np.random.choice(self.exercises)\n",
    "        elif action == 1:\n",
    "            suggestion = np.random.choice(self.songs)\n",
    "        else:\n",
    "            suggestion = np.random.choice(self.quotes)\n",
    "\n",
    "        user_feedback = self.get_user_feedback(suggestion)\n",
    "        reward = self.calculate_reward(user_feedback)\n",
    "\n",
    "        self.state = np.random.uniform(low=0, high=1, size=(3,)).astype(np.float32)\n",
    "        done = False\n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "    def calculate_reward(self, feedback):\n",
    "        if feedback == \"yes\":\n",
    "            return 1.0\n",
    "        elif feedback == \"no\":\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def get_user_feedback(self, suggestion):\n",
    "        # Dummy feedback for testing purposes\n",
    "        feedback = np.random.choice([\"yes\", \"no\"])\n",
    "        return feedback\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Dummy render method\n",
    "        pass\n",
    "\n",
    "# Check the environment\n",
    "env = EmotionExerciseEnv()\n",
    "check_env(env)\n",
    "\n",
    "# Function to load and evaluate the model\n",
    "def load_and_evaluate_model(model_path, env, num_episodes=10):\n",
    "    # Load the model\n",
    "    model = PPO.load(model_path)\n",
    "\n",
    "    # Evaluate the model\n",
    "    total_rewards = 0\n",
    "    episode_lengths = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            env.render()\n",
    "\n",
    "        total_rewards += episode_reward\n",
    "        episode_lengths.append(episode_length)\n",
    "\n",
    "    average_reward = total_rewards / num_episodes\n",
    "    average_length = np.mean(episode_lengths)\n",
    "\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
    "    print(f\"Average Episode Length over {num_episodes} episodes: {average_length}\")\n",
    "\n",
    "# Load and evaluate the model\n",
    "load_and_evaluate_model(\"emotion_exercise_ppo\", env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
